\documentclass[compsoc]{IEEEtran}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{authblk}
\usepackage[english]{babel}
\usepackage{blindtext}

\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{algorithmic,float}
\usepackage{setspace}
\usepackage{amsfonts}
%\usepackage{hyperref}
\graphicspath{ {./images/} }
\usepackage{subfig}
\usepackage{fontspec}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{mathabx}
\usepackage[bottom]{footmisc}
\newfontfamily\listingsfont[Scale=.7]{inconsolata}\usepackage[font=footnotesize,labelfont=bf]{caption}
\captionsetup[algorithm2e]{font=footnotesize}
\usepackage[table,xcdraw]{xcolor}
\usepackage[utf8]{inputenc}
\title{Assignment: Regularization techniques and Autoencoder on Multi-MNIST dataset}
\author{David Bertoldi -- 735213 \\ email: d.bertoldi@campus.unimib.it}
\affil{Department of Informatics, Systems and Communication}
\affil{University of Milano-Bicocca}
\date{October 2022}


\begin{document}

\maketitle 

\section{Inspecting the data}
The data provided consinst of a set of grey-scale images containing 2-digits handwrittend numbers.
The source is a modification of the MNIST dataset: each image is composed by two digits coming from
the original MNIST that may be overlapped in different intensity. This let expand the dataset from
the set of numbers from 0 to 9 to numbers from 1 to 50. \par

Figure \ref{fig:sample} shows the first 16 samples of the training dataset with unique labels.

\begin{figure}[ht!]
\centering                                                                        
\includegraphics[width=3.5in]{sample.png}
\captionsetup{justification=centering}                                                                                                                                   
\caption{Sample of the first 16 unique samples}
\label{fig:sample}
\end{figure}

The data is already divided in training and test datasets. The first present a non-uniform distribution,
as shown in Figure \ref{fig:datahist}. 
Many numbers have very low occurrencies, like 43 with 117 or 17 with 130, against other like 11 with 3112 or 19 with 2890.
As a matter o fact the train dataset has mean $\mu\simeq 1671$ with a standard deviation $\sigma\simeq 1045$, leading to
an unbalanced training for some classes.

\begin{figure}[ht!]
\centering                                                                        
\includegraphics[width=3.5in]{datahist.png}
\captionsetup{justification=centering}                                                                                                                                   
\caption{Histogram of the frequency of samples in the dataset}
\label{fig:datahist}
\end{figure}


Along with the bias of the dataset, some number at first sight are difficult to interpret even for the human mind. For example,
Figure \ref{fig:unread} shows 6 difficult numbers to read. \par

Before training a FFNN using this images, encoded in $28\times39$ matrices with values from $0$ to $255$, we flattened them in
arrays $1\times1092$ and rescaled each value in the continuous interval $[0, 1]$.

\begin{figure}[ht!]
\centering                                                                        
\includegraphics[width=2.5in]{unread.png}
\captionsetup{justification=centering}                                                                                                                                   
\caption{Difficult to read numbers}
\label{fig:unread}
\end{figure}


\section{Preparing the data}
As said in the section before, the dataset is divided into training and test samples. A validation subset is missing and thus
is retrieved from the training set: $20\%$ of the images are randomly used for validation instead of training (along with their labels). \par
About labels, we encoded them in one-hot vectors so that the $1$ are set in the index representing the numerical class.

Another, technical, issue is that \texttt{np\_utils.to\_categorical}, used to create the one-hot vectors, generates 51 classes instead of 50. That's because the function creates as many classes as the highest value inside the input plus one: it takes for granted that we were using a zero-based index. In order
to overcome this, without writing that function from scratch, we subracted 1 to each element of each vector inside the training and test label set.
Obvisously that operation is reverted when trying to predict the values.


\section{Unregularized FFNN}

The aim of this section is to describe a FFNN with less than $100.000$ parameters that is able to classify
with high level of accuracy the numbers from the dataset without any regularization technique. 

\subsection{The network}
Because the number of parameters
are partially determined by the size of the input and output, we tested a FFNN with 2, 3, 4 and 5 hidden layers, but we found that 2-layers model
generalized better over a deeper (and less wide) model. Without entering in the details, the deepest model had 5 layers with 70 to 50 neurons each.\par 


We found a good spot with 80 neurons in the first layer and 50 in the seconds, for a total of 96.660 parameters. 
Figure \ref{fig:noregffnn} shows the architecture of the network used in this section.






\begin{figure}[ht!]
\centering                                                                        
\includegraphics[width=2.5in]{noregffnn.png}
\captionsetup{justification=centering}                                                                                                                                   
\caption{Architecture of the unregularized network}
\label{fig:noregffnn}
\end{figure}

Each hidden neuron uses \texttt{Sigmoid} as activation function. We tried \texttt{LeakyReLU} with $\alpha=0.01$ as well but resulting in a divergence in the model (the validation loss kept tp slighty grow after 15 epochs). The source was probably the explosion of the gradient caused by the function.  \par 
The output layer computes a \texttt{Softmax}, which converts an input vector of real values to an output vector that can be interpreted as categorical probabilities.

\subsection{Training}
The choice of the optimizer was among \emph{SGD}, \emph{RMSProp} and
\emph{Adam}. This selection was influenced also by the initialization of the weights: 
\emph{SGD} performed well with \texttt{GlorotNormal} (Xavier) initializer but not as well as \emph{RMSProp} and
\emph{Adam} with \texttt{HeNormal}. \emph{SGD} with \texttt{HeNormal} resulted in a model that couldn't converge at all.

We tried all of them and chose Adam with learning
rate of $0.001$ , $\beta_1 = 0.9$ and $\beta_2 = 0.999$ because it seemed to
escape better from local minima, converging faster and giving
better accuracy. \par 
Because we are trying to find which images best suits in one of the 50 classes, the best loss function is the \emph{Categorical Cross-Entropy}.\par
The batch size of 256 gave the best results: 512 was another good choice but the convergence was slower and lower values performed worse; that might be
due to the fact that the model needed a good amount of variety of information before every update. But even using mini-batches of 8 the validation accuracy
reached $89\%$.




\begin{figure}[ht!]
\centering                                                                        
\includegraphics[width=3.5in]{../images/noreg/loss-sigmoid-categorical_crossentropy-Adam-50-256.png}
\captionsetup{justification=centering}                                                                                                                                   
\caption{Loss (unregularized)}
\label{fig:loss1}                                                                                                                                                           
\end{figure}


\begin{figure}[ht!]
\centering                                                                        
\includegraphics[width=3.5in]{../images/noreg/accuracy-sigmoid-categorical_crossentropy-Adam-50-256.png}
\captionsetup{justification=centering}                                                                                                                                   
\caption{Categorical accuracy (unregularized)}
\label{fig:acc1}                                                                                                                                          
\end{figure} 

We choose 50 as number of epochs in order to see the effects of missing regularization like \emph{early stopping}, 
even if the model reached an optimal state after 10 epochs.
As shown in Figure \ref{fig:loss1} and \ref{fig:acc1} it is possible to see that the training validation reached $\sim100\%$ which means the model
overfitted so much that almost memorized the training set. 
As metric we used the \emph{categorical accuracy} because calculates the percentage of predicted values that match with true values for one-hot labels.


\subsection{Evaluation}
\begin{figure}[ht!]
\centering                                                                        
\includegraphics[width=3.5in]{noregcm.png}
\captionsetup{justification=centering}                                                                                                                                   
\caption{Confusion matrix of the evaluation of the test set (unregularized network)}
\label{fig:noregcm}                                                                                                                                                           
\end{figure}
The validation accuracy reached $93\%$ after 14 epochs and remained stable till the end. The categorical accuracy 
over the test set reached $89\%$ and can be analyzed with the help of the confusion matrix (Figure \ref{fig:noregcm}). 
The confusion matrix uses a custom colormap (a \emph{viridis} but with the first 10 values set to 0) so that was easier
to hide negligible errors (in this case, less than 10 mismatches). It is noticeable that the model confuses number 50 with 30,
19 with 17, 34 with 39 and others. These errors are not only due to the similarity between their digits, but because
these are the classes with less elements in the training set. So with no surprise the unbalances inside the training set played an
important role.

\par





\section{Regularized FFNN}
The aim of this section is to describe a FFNN with less than $100.000$ parameters that is able to classify
with high level of accuracy the numbers from the dataset with one or more regularization technique. 

\subsection{The network}
In order to better measure the effects of regularization we used the very same architecture as base (2 layers, 80 and 70 neurons).


\subsection{Regularization techniques}
In this section we describe the techiques tested, used or discarded to achieve a lower level of underfitting and secondary to reach a better level of accuracy.

\subsubsection{Data augmentation}
The first attempt of undirect regularization involved filling the gap between the classes by generating new samples for the less populated classes. The procedure, described in Algorithm \ref{alg:aug}, generates for each class as many samples as there are in the percentage $p$ of the difference with the most populated class.
By choosing $p=10\%$ each class diminuishes the gap with the major class by $10\%$. \par
Three techniques were taken in account: \emph{adding noise}, \emph{image shifting} and \emph{image rotation}. The first added a gaussian noise $\mathcal{N}(0, \frac{1}{2})$, the second randomly shifted the image along the 2 axis by 2 in both directions and the third rotated the image by a random angle bewteen $-20^{\circ}$ and $20^{\circ}$. \par
None of the above helped the network: the level of accuracy dropped to $86\%$ and the loss was very high when applying noise ($\geq 0.6$). Image roation alone is the only one that didn't make the accuracy worse. For these reasons data augmentation was discarded even if theoretically speaking filling the gap between samples makes sense. Probably the model already recognized single features for those samples so that adding similar samples just increased redundancy.


\subsubsection{L1 and L2}
\emph{L1} and \emph{L2}

\subsubsection{Dropout}
Dropout turned out to be a good choice: it dropped the overfitting without losing too much accuracy. We chose a drop out probability for each layer of $10\%$ because the network wasn't too big and higher probabilities gave worse results. This strategy helped the network to ignore certain features of the images or the $0s$ of the matrices (black spaces). A representation of the network can be found in Figure \ref{fig:regffnn}. 

\begin{figure}[ht!]
\centering                                                                        
\includegraphics[width=2.5in]{regffnn.png}
\captionsetup{justification=centering}                                                                                                           
\caption{Architecture of the network with dropout}
\label{fig:regffnn}
\end{figure}

\subsubsection{Early stopping}
Another game changer was the application of \emph{early stopping} on the validation, with a patience of 10 epochs and $\delta = 0.05$. This greately reduced the overfitting and because the model converged after 15-20 epochs we restored the weights to the last best snapshot ($20^{\text{th}}$ epoch).








\begin{figure}[ht!]
\centering                                                                        
\includegraphics[width=3.5in]{../images/reg/loss-LeakyReLU-NoneType-categorical_crossentropy-Adam-50-256-0.1.png}
\captionsetup{justification=centering}                                                                                                                              
\caption{Loss (unregularized)}
\label{fig:loss2}                                                                                                                                                           
\end{figure}


\begin{figure}[ht!]
\centering                                                                        
\includegraphics[width=3.5in]{../images/reg/accuracy-LeakyReLU-NoneType-categorical_crossentropy-Adam-50-256-0.1.png}
\captionsetup{justification=centering}                                                                                                                            
\caption{Categorical accuracy (unregularized)}
\label{fig:acc2}                                                                                                                                          
\end{figure} 


\subsection{Evaluation}
\begin{figure}[ht!]
\centering                                                                        
\includegraphics[width=3.5in]{regcm.png}
\captionsetup{justification=centering}
\caption{Confusion matrix of the evaluation of the test set (regularized network)}
\label{fig:regcm}
\end{figure}


\begin{algorithm}[h]
\SetAlgoLined
\KwData{\emph{$X$ training samples}, \emph{$Y$} training labels, $p \in \mathbb{N}$}
\KwResult{\emph{$X$}, \emph{$Y$} }
$l=|Y|$\;
$C \gets \{\}$\;
\For{$i \gets 0$ \textbf{to} $l$}{
\eIf{$Y_i \in C$}{
$C_i \gets C_i + 1$\;
}{
$C_i \gets 1$\;
}
}


$m \gets \max_k C_k$\;

\For{$(k,v) \in C$}{
$S = \{e \in X : e = k\}$\;
$g \gets \left \lfloor \frac{m - v}{p} \right \rfloor$\;
\For{$i \gets 0$ \textbf{to} $g$}{
$x \gets augment(S_{random})$\;
$\text{append } x \text{ to } X$\;
$\text{append } k \text{ to } Y$\;
}
}
\Return $shuffle(X, y)$\;

\caption{Data augmentation algorithm}\label{alg:aug}
\end{algorithm}

\end{document}









