\documentclass[compsoc]{IEEEtran}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{authblk}
\usepackage[english]{babel}
\usepackage{blindtext}
%\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
%\usepackage{algorithmic,float}
\usepackage{setspace}
\usepackage{amsfonts}
%\usepackage{hyperref}
\graphicspath{ {../images/} }
\usepackage{subfig}
\usepackage{fontspec}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{mathabx}
\usepackage[bottom]{footmisc}
\newfontfamily\listingsfont[Scale=.7]{inconsolata}\usepackage[font=footnotesize,labelfont=bf]{caption}
%\captionsetup[algorithm2e]{font=footnotesize}
\usepackage[table,xcdraw]{xcolor}
\usepackage[utf8]{inputenc}
\title{Assignment: Transfer Learning on Intel Image Classification}
\author{David Bertoldi -- 735213 \\ email: d.bertoldi@campus.unimib.it}
\affil{Department of Informatics, Systems and Communication}
\affil{University of Milano-Bicocca}
\date{October 2022}


\begin{document}

\maketitle 



\section{Dataset}\label{sec:insp}
The chosen dataset is called Intel$^\circledR$ Image Classification and it was initially published on Analytics Vidhya by Intel$^\circledR$ to host an image classification challenge to promote OpenVINO™, a toolkit or optimizing and deploying AI inference \cite{site1}\cite{site2}. \par

The dataset contains images of natural scenes around the world and they belong to $6$ classes: buildings, forests, glaciers, mountains, sea and streets. The images are of size $150\times150$px and can be colored ($3$ channels, RGB) or rarely in grayscale (still with $3$ channels). Figure \ref{fig:samples} shows $16$ entries of the training dataset. \par
There is a total of $\sim 24\,000$ images, divided into Train ($\sim14\,000$), Test ($\sim3\,000$) and Prediction ($\sim7\,000$) folders. The last one does not contain labels and it is intended for unsupervised learning and it will be ignored in this work.



\begin{figure}[ht!]
\centering                                                                        
\includegraphics[width=2.5in]{../images/samples.png}
\captionsetup{justification=centering}                                                                                         
\caption{$16$ random entries of the train dataset}
\label{fig:samples}                                                                                                                               
\end{figure}

The distribution of the images across the classes follows a uniform distribution $U(\mu, \sigma)$: in the train set each class has an average $\mu = 2\,339$ images with $\sigma = 105.45$ and in the test set $\mu=500$ and $\sigma=36.92$. We didn't find any bias inside the dataset since all the classes are equally populated and so we didn't applied any kind of data augmentation on particular classes for rebalacing.


\begin{figure}[ht!]
\centering                                                                        
\includegraphics[width=3in]{../images/data.png}
\captionsetup{justification=centering}                                                                                         
\caption{16 entries of the train dataset}
\label{fig:data}                                                                                                                               
\end{figure}

The $6$ classes are encoded with numbers $0$ to $5$ and Table \ref{tab:encode} shows the mapping between the numerical and nominative form.



\begin{table}[ht!]
\centering
\begin{tabular}{|l|l|}
\hline
\rowcolor[HTML]{9698ED} 
{\color[HTML]{FFFFFF} \textbf{Number}} & {\color[HTML]{FFFFFF} \textbf{Class}} \\ \hline
0                                      & Building                              \\ \hline
1                                      & Forest                                \\ \hline
2                                      & Glacier                               \\ \hline
3                                      & Mountain                              \\ \hline
4                                      & Sea                                   \\ \hline
5                                      & Street                                \\ \hline
\end{tabular}
\caption{Mapping between numbers and names}
\label{tab:encode}
\end{table}



\section{The model} 
The chosen dataset presented similarieties with ImageNet: the $6$ classes of Intel$^\circledR$ Image Classification are scattered and distributed in the $1\,000$ classes of ImageNet. For this reason a pretrained model on ImageNet speeded up the learning process. The chosen model is VGG16, a $16-$layers deep CNN proposed by Karen Simonyan and Andrew Zisserman at the University of Oxford\cite{vgg16}. Figure \ref{fig:vgg16} shows an overview of its architecture.


\begin{figure}[ht!]
\centering                                                                        
\includegraphics[width=3.5in]{../images/vgg16.png}
\captionsetup{justification=centering}                                                                                         
\caption{Architecture of VGG16 with the cuts applied in this work}
\label{fig:vgg16}                                                                                                                               
\end{figure}

In this work we proposed different cuts to the network and feeded its outputs to a "classic" machine learning model, a SVM, and benchmark the performance of the hybrid architecture. \par

The chosen cuts were after the first dense layer (\texttt{fc1}), after the fourth pooling layer (\texttt{block4\_pool}) 
and after the third pooling layer (\texttt{block3\_pool}). The different cuts led to different challenges, such as the high dimensionality of the features.

\begin{table}[]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|c|l|}
\hline
\rowcolor[HTML]{9698ED} 
{\color[HTML]{FFFFFF} \textbf{Cut}} & {\color[HTML]{FFFFFF} \textbf{Trainable parameters}} & {\color[HTML]{FFFFFF} \textbf{Dimension}} \\ \hline
\texttt{fc1}                                 & $117\,479\,232$  & $1\times1\times4096=\mathbf{4\,096}$                             \\ \hline
\texttt{block4\_pool}                        & $7\,635\,264$  & $14\times14\times512 = \mathbf{100\,352}$                      \\ \hline
\texttt{block3\_pool}                        & $1\,735\,488$  & $28\times28\times256= \mathbf{200\,704}$                         \\ \hline
\end{tabular}}
\caption{Number of VGG16's trainable parameters and dimensions of the extracted features at each cutting point}
\label{tab:dims}
\end{table}

As the cuts approached the input, the number of trainable parameters decreased exponentially; however the representation of the features will have an increasingly higher dimension. The higher dimensionality affects the training performance of the SVM and a fine tuning on the management of the memory. For this reason we used less samples during the training phase as the dimensionality increased, increasing the risk of underfitting. Table \ref{tab:dims} describe the details of the problem.

\bibliographystyle{ieeetr}
\bibliography{Bibliography}

\begin{thebibliography}{9}

\bibitem{site1} 
Practice Problem: Intel Scene Classification Challenge \\
\texttt{https://datahack.analyticsvidhya.com/contest/practice-problem-intel-scene-classification-challe}

\bibitem{site2} 
OpenVINO™ documentation \\
\texttt{https://docs.openvino.ai/latest/index.html}

\bibitem{vgg16}
\emph{Very Deep Convolutional Networks for Large-Scale Image Recognition} \\
Karen Simonyan, Andrew Zisserman \\
\texttt{https://doi.org/10.48550/arXiv.1409.1556}

\end{thebibliography}

\end{document}









